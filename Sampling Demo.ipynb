{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import importlib\n",
    "import functools\n",
    "import itertools\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "import os\n",
    "import io\n",
    "\n",
    "import sampling as sampling\n",
    "import datasets\n",
    "import models\n",
    "\n",
    "from models.ema import ExponentialMovingAverage\n",
    "from utils import restore_checkpoint\n",
    "from losses import get_optimizer\n",
    "\n",
    "from models import utils as mutils\n",
    "from models import ncsnpp\n",
    "\n",
    "from sde_lib import VESDE\n",
    "from sampling import (ReverseDiffusionPredictor, \n",
    "                      LangevinCorrector, \n",
    "                      EulerMaruyamaPredictor,\n",
    "                      AncestralSamplingPredictor,\n",
    "                      NoneCorrector, \n",
    "                      NonePredictor,\n",
    "                      AnnealedLangevinDynamics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        self.act = nn.ReLU()\n",
    "        self.pool = nn.AvgPool2d(2, 2)\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, 1, 1) # Size 16\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1) # Size 8\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, 1, 1) # Size 4\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, 1, 1) # Size 2\n",
    "        self.fc = nn.Linear(1024, 1000)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.act(self.conv1(x)))\n",
    "        x = self.pool(self.act(self.conv2(x)))\n",
    "        x = self.pool(self.act(self.conv3(x)))\n",
    "        x = self.pool(self.act(self.conv4(x)))\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class SimpleCNN256(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SimpleCNN256, self).__init__()\n",
    "        \n",
    "        self.act = nn.ReLU()\n",
    "        self.pool2 = nn.AvgPool2d(2, 2)\n",
    "        self.pool4 = nn.AvgPool2d(2, 4)\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, 1, 1) # Size 64\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1) # Size 16\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, 1, 1) # Size 4\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, 1, 1) # Size 2\n",
    "        self.fc = nn.Linear(1024, 1000)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool4(self.act(self.conv1(x)))\n",
    "        x = self.pool4(self.act(self.conv2(x)))\n",
    "        x = self.pool4(self.act(self.conv3(x)))\n",
    "        x = self.pool2(self.act(self.conv4(x)))\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def get_sigma(y, sigma_max):\n",
    "    sigma_min = torch.tensor([0.01]).to(y.device)\n",
    "    sigma_max = torch.tensor([sigma_max]).to(y.device)\n",
    "    ts = torch.linspace(1.0, 1e-3, 1000).to(y.device)\n",
    "    ss = sigma_min * (sigma_max / sigma_min).to(y.device) ** ts\n",
    "    return ss[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "-reedYgCU79v"
   },
   "outputs": [],
   "source": [
    "dataset = 'celebahq'\n",
    "if dataset.lower() == 'cifar10':\n",
    "    from configs.ve import cifar10_ncsnpp_continuous as configs\n",
    "    ckpt_filename = \"exp/ve/cifar10_ncsnpp_continuous/checkpoint_24.pth\"\n",
    "    config = configs.get_config() \n",
    "    sampling_eps = 1e-5\n",
    "    twd_eps = 1e-02\n",
    "    snr = 0.16\n",
    "    batch_size = 200\n",
    "    FID_N = 50000\n",
    "elif dataset.lower() == 'celebahq':\n",
    "    from configs.ve import celebahq_256_ncsnpp_continuous as configs\n",
    "    ckpt_filename = \"exp/ve/celebahq_256_ncsnpp_continuous/checkpoint_48.pth\"\n",
    "    config = configs.get_config() \n",
    "    sampling_eps = 1e-5\n",
    "    twd_eps = 1e-3\n",
    "    snr = 0.075\n",
    "    batch_size = 20\n",
    "    FID_N = 10000\n",
    "\n",
    "config.training.batch_size = batch_size\n",
    "config.eval.batch_size = batch_size\n",
    "\n",
    "sigmas = mutils.get_sigmas(config)\n",
    "scaler = datasets.get_data_scaler(config)\n",
    "inverse_scaler = datasets.get_data_inverse_scaler(config)\n",
    "score_model = mutils.create_model(config)\n",
    "\n",
    "optimizer = get_optimizer(config, score_model.parameters())\n",
    "ema = ExponentialMovingAverage(score_model.parameters(), decay=config.model.ema_rate)\n",
    "state = dict(step=0, optimizer=optimizer, model=score_model, ema=ema)\n",
    "state = restore_checkpoint(ckpt_filename, state, config.device)\n",
    "ema.copy_to(score_model.parameters())\n",
    "\n",
    "img_size = config.data.image_size\n",
    "channels = config.data.num_channels\n",
    "shape = (batch_size, channels, img_size, img_size)\n",
    "\n",
    "if 'cifar10' in dataset.lower():\n",
    "    net = SimpleCNN().cuda()\n",
    "    ckpt = torch.load('./logs/CIFAR10/200.pt')\n",
    "    net.load_state_dict(ckpt['net'])\n",
    "elif 'celebahq' in dataset.lower():\n",
    "    net = SimpleCNN256().cuda()\n",
    "    ckpt = torch.load('./logs/CELEBAHQ/100.pt')\n",
    "    net.load_state_dict(ckpt['net'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ode_sampler = functools.partial(sampling.get_ode_sampler,\n",
    "                                    shape=shape,\n",
    "                                    inverse_scaler=inverse_scaler,\n",
    "                                    denoise=False,\n",
    "                                    rtol=1e-5,\n",
    "                                    atol=1e-5,\n",
    "                                    eps=sampling_eps,\n",
    "                                    device=config.device)\n",
    "\n",
    "get_pc_sampler = functools.partial(sampling.get_pc_sampler,\n",
    "                                   shape=shape,\n",
    "                                   predictor=ReverseDiffusionPredictor,\n",
    "                                   # predictor=EulerMaruyamaPredictor,\n",
    "                                   # predictor=AncestralSamplingPredictor,\n",
    "                                   # corrector=LangevinCorrector,\n",
    "                                   corrector=None,\n",
    "                                   inverse_scaler=inverse_scaler,\n",
    "                                   snr=snr,\n",
    "                                   # n_steps=1,\n",
    "                                   n_steps=0,\n",
    "                                   probability_flow=False,\n",
    "                                   continuous=config.training.continuous,\n",
    "                                   eps=sampling_eps,\n",
    "                                   device=config.device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def karras_alg1(score_model, x_init, sigma_max, N):\n",
    "    rho = 7\n",
    "    sigma_min = torch.tensor([0.01]).float().to(x_init.device)\n",
    "    sigma_max = sigma_max.to(x_init.device)\n",
    "    ls = torch.linspace(0.0, 1.0, N-1).reshape(1,N-1).to(x_init.device)\n",
    "    sigma_root = ls * (sigma_min.pow(1/rho) - sigma_max.pow(1/rho)).reshape(-1,1) + sigma_max.pow(1/rho).reshape(-1,1)\n",
    "    sigma = sigma_root.pow(rho)\n",
    "    sigma = torch.cat([sigma,torch.zeros(size=[x_init.shape[0],1]).to(x_init.device)], dim=1)\n",
    "    \n",
    "    score_model.eval()\n",
    "    x = x_init.detach().clone()\n",
    "    for i in range(N - 1):\n",
    "        d1 = -(sigma[:,i]).reshape(-1,1,1,1) * score_model(x, sigma[:,i])\n",
    "        x_hat = x + (sigma[:,i+1] - sigma[:,i]).reshape(-1,1,1,1) * d1\n",
    "        if i < N-2:\n",
    "            d2 = -(sigma[:,i+1]).reshape(-1,1,1,1) * score_model(x_hat, sigma[:,i+1])\n",
    "            x.data = x + 0.5 * (sigma[:,i+1] - sigma[:,i]).reshape(-1,1,1,1) * (d1 + d2)\n",
    "        else:\n",
    "            x.data = x_hat\n",
    "    return x.detach().clone(), 2*(N-1)-1\n",
    "\n",
    "@torch.no_grad()\n",
    "def karras_alg2(score_model, x_init, sigma_max, N, noise=0.007):\n",
    "    rho = 7\n",
    "    sigma_min = torch.tensor([0.01]).float().to(x_init.device)\n",
    "    sigma_max = sigma_max.to(x_init.device)\n",
    "    ls = torch.linspace(0.0, 1.0, N-1).reshape(1,N-1).to(x_init.device)\n",
    "    sigma_root = ls * (sigma_min.pow(1/rho) - sigma_max.pow(1/rho)).reshape(-1,1) + sigma_max.pow(1/rho).reshape(-1,1)\n",
    "    sigma = sigma_root.pow(rho)\n",
    "    sigma = torch.cat([sigma,torch.zeros(size=[x_init.shape[0],1]).to(x_init.device)], dim=1)\n",
    "    \n",
    "    S_churn = 80\n",
    "    S_tmin = 0.05\n",
    "    S_tmax = 1.0\n",
    "    S_noise = 1 + noise\n",
    "    \n",
    "    score_model.eval()\n",
    "    x = x_init.detach().clone()\n",
    "    for i in range(N-1):\n",
    "        eps = S_noise * torch.randn_like(x).to(x_init.device)\n",
    "        cond = (sigma[:,i] >= S_tmin) * (sigma[:,i] <= S_tmax)\n",
    "        gamma = np.minimum(S_churn/N, np.sqrt(2)-1) * cond\n",
    "        sigma_hat = (1 + gamma) * sigma[:,i]\n",
    "        x_hat = x + (sigma_hat**2 - sigma[:,i]**2).sqrt().reshape(-1,1,1,1) * eps\n",
    "        d1 = - sigma_hat.reshape(-1,1,1,1) * score_model(x_hat, sigma_hat)\n",
    "        x_next = x_hat + (sigma[:,i+1] - sigma_hat).reshape(-1,1,1,1) * d1\n",
    "        if i < N-2:\n",
    "            d2 = - sigma[:,i+1].reshape(-1,1,1,1) * score_model(x_next, sigma[:,i+1])\n",
    "            x.data = x_hat + 0.5 * (sigma[:,i+1] - sigma_hat).reshape(-1,1,1,1) * (d1 + d2)\n",
    "        else:\n",
    "            x.data = x_next\n",
    "    return x.detach().clone(), 2*(N-1)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DLG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for N, eta, skp in [(20, 0.8, 10)]:\n",
    "\n",
    "    save_dir = './FID/{}/kar2_{}_dmcmc_{:.1e}_{}'.format(dataset.upper(), N, eta, skp)\n",
    "    print('save dir : {}'.format(save_dir))\n",
    "    contin = False\n",
    "    \n",
    "    score_model.eval()\n",
    "    resume = contin and os.path.isdir(save_dir)\n",
    "    if resume:\n",
    "        S = len(os.listdir(save_dir)) - 1\n",
    "        n_samples = math.ceil((FID_N - S) / batch_size)\n",
    "        ckpt = torch.load(save_dir + '/ckpt.pt')\n",
    "        x = ckpt['x'].cuda()\n",
    "        s = ckpt['s'].cuda()\n",
    "    else:\n",
    "        if os.path.isdir(save_dir):\n",
    "            for f in os.listdir(save_dir):\n",
    "                os.remove(os.path.join(save_dir,f))\n",
    "        else:\n",
    "            os.makedirs(save_dir)\n",
    "        \n",
    "        S = 0\n",
    "        n_samples = math.ceil(FID_N/batch_size)\n",
    "        # Generating MCMC chain initialization points\n",
    "        with torch.no_grad():\n",
    "            x = config.model.sigma_max * torch.randn(size=shape).cuda()\n",
    "            s = config.model.sigma_max * torch.ones(batch_size).cuda()\n",
    "\n",
    "            if 'cifar10' in dataset:\n",
    "                sampling_fn = functools.partial(karras_alg2, sigma_max=s.cpu(), N=20)\n",
    "                x, _ = sampling_fn(score_model, x)\n",
    "                x = x + 0.5 * torch.randn_like(x).cuda()\n",
    "                s = get_sigma(net(x).argmax(dim=1), config.model.sigma_max)\n",
    "                \n",
    "                for _ in range(10):\n",
    "                    x.data = x + 0.5 * 0.5 * score_model(x, s) + np.sqrt(0.5) * torch.randn_like(x)\n",
    "                    s.data = get_sigma(net(x).argmax(dim=1), config.model.sigma_max)\n",
    "\n",
    "                for _ in range(10):\n",
    "                    x.data = x + 0.5 * eta * score_model(x, s) + np.sqrt(eta) * torch.randn_like(x)\n",
    "                    s.data = get_sigma(net(x).argmax(dim=1), config.model.sigma_max)\n",
    "            else:\n",
    "                sampling_fn = functools.partial(karras_alg2, sigma_max=s.cpu(), N=20)\n",
    "                x, _ = sampling_fn(score_model, x)\n",
    "                x = x + 0.5 * torch.randn_like(x).cuda()\n",
    "                s = get_sigma(net(x).argmax(dim=1), config.model.sigma_max)\n",
    "\n",
    "                for _ in range(50):\n",
    "                    x.data = x + 0.5 * 1.6 * score_model(x, s) + np.sqrt(1.6) * torch.randn_like(x)\n",
    "                    s.data = get_sigma(net(x).argmax(dim=1), config.model.sigma_max)\n",
    "\n",
    "                for _ in range(20):\n",
    "                    x.data = x + 0.5 * eta * score_model(x, s) + np.sqrt(eta) * torch.randn_like(x)\n",
    "                    s.data = get_sigma(net(x).argmax(dim=1), config.model.sigma_max)\n",
    "\n",
    "    # Sampling\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(int(n_samples))):\n",
    "            # Langevin Gibbs using n_skip NFE\n",
    "            x_min = x.detach().clone()\n",
    "            s_min = s.detach().clone()\n",
    "            for _ in range(skp):\n",
    "                x.data = x + 0.5 * eta * score_model(x, s) + np.sqrt(eta) * torch.randn_like(x) # x ~ p(x|s)\n",
    "                s.data = get_sigma(net(x).argmax(dim=1), config.model.sigma_max) # s ~ p(s|x)\n",
    "                \n",
    "                # Choose MCMC samples of smallest noise levels\n",
    "                cond = (s < s_min)\n",
    "                x_min.data = cond.reshape(-1,1,1,1) * x + (~cond).reshape(-1,1,1,1) * x_min\n",
    "                s_min.data = cond * s + (~cond) * s_min\n",
    "            \n",
    "            # Define reverse-SDE/ODE integrator\n",
    "            sde = VESDE(sigma_min=config.model.sigma_min, sigma_max=s_min.cpu(), N=N) # Define SDE\n",
    "            # sampling_fn = get_ode_sampler(sde) # ODE integrator\n",
    "            # sampling_fn = get_pc_sampler(sde) # PC integrator\n",
    "            # sampling_fn = functools.partial(karras_alg1, sigma_max=s_min.cpu(), N=N) # KAR1 integrator\n",
    "            sampling_fn = functools.partial(karras_alg2, sigma_max=s_min.cpu(), N=N) # KAR2 integrator\n",
    "            \n",
    "            # Run denoising using n_den NFE\n",
    "            x_den, n = sampling_fn(score_model, x_min)\n",
    "            \n",
    "            # Tweedie's denoising formula\n",
    "            x_den = x_den + twd_eps**2 * score_model(x_den, twd_eps * torch.ones_like(s).cuda())\n",
    "            \n",
    "            ckpt = {\n",
    "                'x' : x.detach().clone(),\n",
    "                's' : s.detach().clone(),\n",
    "            }\n",
    "            torch.save(ckpt, save_dir + '/ckpt.pt')\n",
    "\n",
    "            for j in range(batch_size):\n",
    "                if int(batch_size*i+j+S) >= FID_N:\n",
    "                    break\n",
    "                img = torch.clamp(x_den[j], 0.0, 1.0)\n",
    "                img = Image.fromarray((255 * img.permute(1,2,0).detach().cpu().numpy()).astype(np.uint8))\n",
    "                img.save(save_dir + '/{}.jpg'.format(int(batch_size*i+j+S)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in [201]:\n",
    "\n",
    "    save_dir = './FID/{}/kar2_{}_vanilla'.format(dataset.upper(), N)\n",
    "    print('save dir : {}'.format(save_dir))\n",
    "    n_samples = math.ceil(FID_N/batch_size)\n",
    "    contin = False\n",
    "\n",
    "    if os.path.isdir(save_dir):\n",
    "        if contin:\n",
    "            S = len(os.listdir(save_dir))\n",
    "        else:\n",
    "            for f in os.listdir(save_dir):\n",
    "                os.remove(os.path.join(save_dir,f))\n",
    "            S = 0\n",
    "    else:\n",
    "        os.makedirs(save_dir)\n",
    "        S = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(n_samples)):\n",
    "            x = (config.model.sigma_max * torch.randn(size=shape)).cuda()\n",
    "            s = (config.model.sigma_max * torch.ones(batch_size)).cuda()\n",
    "            # sde = VESDE(sigma_min=config.model.sigma_min, sigma_max=s.cpu(), N=N)\n",
    "            # sampling_fn = get_ode_sampler(sde)\n",
    "            # sampling_fn = get_pc_sampler(sde)\n",
    "            # sampling_fn = functools.partial(karras_alg1, sigma_max=s.cpu(), N=N)\n",
    "            sampling_fn = functools.partial(karras_alg2, sigma_max=s.cpu(), N=N)\n",
    "            x_den, n = sampling_fn(score_model, x)\n",
    "            x_den = x_den + twd_eps**2 * score_model(x_den, twd_eps * torch.ones_like(s).cuda())\n",
    "\n",
    "            for j in range(batch_size):\n",
    "                if int(batch_size*i+j+S) >= FID_N:\n",
    "                    break\n",
    "                img = torch.clamp(x_den[j], 0.0, 1.0)\n",
    "                img = Image.fromarray((255 * img.permute(1,2,0).detach().cpu().numpy()).astype(np.uint8))\n",
    "                img.save(save_dir + '/{}.jpg'.format(int(batch_size*i+j+S)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Score SDE demo PyTorch",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
